<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Virtual Marker</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:title" content="3D Human Mesh Estimation from Virtual Markers" />
    <meta property="og:description" content="Inspired by the success of volumetric 3D pose estimation, some recent human mesh estimators propose to estimate 3D skeletons as intermediate representations, from which, the dense 3D meshes are regressed by exploiting the mesh topology. However, body shape information is lost in extracting skeletons, leading to mediocre performance. The advanced motion capture systems solve the problem by placing dense physical markers on the body surface, which allows to extract realistic meshes from their non-rigid motions. However, they cannot be applied to wild images without markers. In this work, we present an intermediate representation, named virtual markers, which learns 64 landmark keypoints on the body surface based on the large-scale mocap data in a generative style, mimicking the effects of physical markers. The virtual markers can be accurately detected from wild images and can reconstruct the intact meshes with realistic shapes by simple interpolation. Our approach outperforms the state-of-the-art methods on three datasets. In particular, it surpasses the existing methods by a notable margin on the SURREAL dataset, which has diverse body shapes."
    />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="3D Human Mesh Estimation from Virtual Markers" />
    <meta name="twitter:description" content="Inspired by the success of volumetric 3D pose estimation, some recent human mesh estimators propose to estimate 3D skeletons as intermediate representations, from which, the dense 3D meshes are regressed by exploiting the mesh topology. However, body shape information is lost in extracting skeletons, leading to mediocre performance. The advanced motion capture systems solve the problem by placing dense physical markers on the body surface, which allows to extract realistic meshes from their non-rigid motions. However, they cannot be applied to wild images without markers. In this work, we present an intermediate representation, named virtual markers, which learns 64 landmark keypoints on the body surface based on the large-scale mocap data in a generative style, mimicking the effects of physical markers. The virtual markers can be accurately detected from wild images and can reconstruct the intact meshes with realistic shapes by simple interpolation. Our approach outperforms the state-of-the-art methods on three datasets. In particular, it surpasses the existing methods by a notable margin on the SURREAL dataset, which has diverse body shapes."
    />

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>



<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>3D Human Mesh Estimation from Virtual Markers</b><br>
                <small>
                        CVPR 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://shirleymaxx.github.io/", target="_blank">Xiaoxuan Ma</a><sup>1</sup>,
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank">Jiajun Su</a><sup>1</sup>,
                    </li>
                    <li>
                        <a href="https://www.chunyuwang.org/", target="_blank">Chunyu Wang</a><sup>3 *</sup>, 
                    </li>
                    <li>
                        <a href="https://wentao.live/", target="_blank">Wentao Zhu</a><sup>1</sup>,
                    </li>
                    <li>
                        <a href="https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm", target="_blank">Yizhou Wang</a><sup>1,2,4</sup>
                    </li>
                </ul>
                <p><sup>1</sup>School of Computer Science, Center on Frontiers of Computing Studies, Peking University <br> 
                    <sup>2</sup>Inst. for Artificial Intelligence, Peking University &nbsp;&nbsp;
                    <sup>3</sup>Microsoft Research Asia &nbsp;&nbsp;
                    <sup>4</sup>Nat'l Eng. Research Center of Visual Technology <br>
                    <sup>*</sup>Corresponding author
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified" style="margin-top:10px">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.pdf" target="_blank">
                            <strong><font size="+1">[Paper]</font></strong>
                    </a>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://github.com/ShirleyMaxx/VirtualMarker" target="_blank">
                            <strong><font size="+1">[Code]</font></strong>
                    </a>
                    &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ma_3D_Human_Mesh_CVPR_2023_supplemental.pdf" target="_blank">
                            <strong><font size="+1">[Supplementary]</font></strong>
                    </a>
                </ul>
            </div>
        </div>



        <div class="row ">
            <div class="col-md-8 col-md-offset-2 text-center">
                <image src="images/cvpr23_virtualmarker_crop.gif" width="70%">
                </image>
            </div>
            <div class="col-md-8 col-md-offset-2 ">
                <p class="text-center ">
                    We introduce a novel representation named <i>Virtual Markers</i> for 3D human mesh estimation.
                </p>
            </div>
        </div>


        <div class="row ">
            <div class="col-md-8 col-md-offset-2 ">
                <h3>
                    <b>Abstract</b>
                </h3>
                <p class="text-justify ">
                    Inspired by the success of volumetric 3D pose estima- tion, some recent human mesh estimators propose to esti- mate 3D skeletons as intermediate representations, from which, the dense 3D meshes are regressed by exploiting the mesh topology. However, body shape information is lost in extracting skeletons, leading to mediocre performance. The advanced motion capture systems solve the problem by placing dense physical markers on the body surface, which allows to extract realistic meshes from their non-rigid mo- tions. However, they cannot be applied to wild images without markers. In this work, we present an intermedi- ate representation, named virtual markers, which learns 64 landmark keypoints on the body surface based on the large-scale mocap data in a generative style, mimicking the effects of physical markers. The virtual markers can be ac- curately detected from wild images and can reconstruct the intact meshes with realistic shapes by simple interpolation. Our approach outperforms the state-of-the-art methods on three datasets. In particular, it surpasses the existing meth- ods by a notable margin on the SURREAL dataset, which has diverse body shapes.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Video</b>
                </h3>
                <div class="text-center">
                    <div class="embed-responsive embed-responsive-16by9">
                        <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/je2gNUiYl2c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <br> 

        <div class="row ">
            <div class="col-md-8 col-md-offset-2 ">
                <h3>
                    <b>Results on natural videos</b>
                </h3>
                <div class=" ">
                    <div style="position:relative text-center">
                        <div style="display:inline;"><video class="vid1" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo1.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid2" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo2.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid3" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo3.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid4" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo4.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid5" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo5.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid6" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo6.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid7" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo7.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid8" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo8.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid9" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo9.mp4" type="video/mp4"></video></div>
                        <div style="display:inline;"><video class="vid10" width="49.5%" height="100%" autoplay loop muted controls src="images/demos/demo10.mp4" type="video/mp4"></video></div>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <br>

        <div class="row ">
            <div class="col-md-8 col-md-offset-2 ">
                <h3>
                    <b>Comparison on the SURREAL dataset</b>
                </h3>

                <br>
                <a href="images/cvpr23_surreal.png"><img src="images/cvpr23_surreal.png" width="100%" style="border-style: none"></a>

                <br>
                <br>

                <h3>
                    <b>Comparison on the 3DPW dataset</b>
                </h3>

                <br>
                <a href="images/cvpr23_pw3d_compare.png"><img src="images/cvpr23_pw3d_compare.png" width="100%" style="border-style: none"></a>
 
            </div>
        </div>

        <br>
        <br>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Citation</b>
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@InProceedings{Ma_2023_CVPR,
    author    = {Ma, Xiaoxuan and Su, Jiajun and Wang, Chunyu and Zhu, Wentao and Wang, Yizhou},
    title     = {3D Human Mesh Estimation From Virtual Markers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {534-543}
}</textarea>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    Acknowledgements
                </h3> -->
                <p class="text-justify">
                    Template courtesy of <a href="https://jonbarron.info/mipnerf360/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>


</html>